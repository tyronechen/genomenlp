# -*- coding: utf-8 -*-
"""TyagiLab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j2BmFRoiV9N2Oouy-gC4wh1FLL_JH3AL
"""

def parse_fasta(fh):
    fs=[]
    fss=[]
    seq=''
    for ln in fh:
        if ln[0]=='A' or ln[0]=='T' or ln[0]=='G' or ln[0]=='C':
           seq=seq+ln[:-2]
        else:
           fs.append(seq)
           seq=''
    for element in fs:
      if element!='':
        fss.append(element)
      else:
        fss=fss
    return fss

fh=open('/content/Human_promoters.fasta')
#fh1=open('/Human_promoters_1.fa')
fh2=open('/content/Human_promoters-BERT.fa')
#sequence=parse_fasta(fh)
#sequence1=parse_fasta(fh1)
input=parse_fasta(fh2)
input1=parse_fasta(fh)
print(input)

#k-mer
def getKmers(sequence, size):
    return [sequence[x:x+size].upper() for x in range(len(sequence) - size + 1)]
def getKmersList(sequences,size):
  seq_list=[]
  for i in range(0,len(sequences)):
    seq_list.append(getKmers(sequences[i],size))
  return seq_list

f=getKmersList(input,3)
print(f)

#space after k-mer
def spacings(sequence,size,index):
  initial=''
  for element in getKmersList(sequence,size)[index]:
    initial=initial+element+' '
  return initial[:-1]
spacings(input,3,0)

def sentence_in_list(sequence,size,index):
  sil=[]
  for element in getKmersList(sequence,size)[index]:
    sil.append([element])
  return sil
sentence_in_list(input,3,0)

# Word2vec function
def word2vec(sequences,size):
  sentences=getKmersList(sequences,size)
  from gensim.models import Word2Vec
  # train model
  model = Word2Vec(sentences, min_count=1)
  words = list(model.wv.vocab)
  #summarizing model
  print(words)
  print(model['AGAG'])
  # save model
  model.save('model.bin')
  # load model
  new_model = Word2Vec.load('model.bin')
  print(new_model)
#data
sequences=input
size=4
# function calling
word2vec(sequences,size)

# tf-idf
def tfidf(sequence,size,index): #tfidf(d1, d2)
   data = spacings(sequence,size,index)
   # import required module
   from sklearn.feature_extraction.text import TfidfVectorizer
   # merge documents into a single corpus
   string =[data] #[d1, d2]
   # create object
   tfidf = TfidfVectorizer()
   # get tf-df values
   result = tfidf.fit_transform(string)
   # get idf values
   print('\nidf values:')
   for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):
	   print(ele1, ':', ele2)
   # get indexing
   print('\nWord indexes:')
   print(tfidf.vocabulary_)
   # display tf-idf values
   print('\ntf-idf value:')
   print(result)
   # in matrix form
   print('\ntf-idf values in matrix form:')
   print(result.toarray())
# data
sequence=input
size=4
index=0
tfidf(sequence,size,index) # tfidf(d1, d2)

# ordinal method of encoding function
def ordinalen(sequence,size,index):
   doc=sentence_in_list(sequence,size,index)
   from numpy import asarray
   from sklearn.preprocessing import OrdinalEncoder
   data=asarray(doc)
   print(data)
   encoder = OrdinalEncoder()
   result = encoder.fit_transform(data)
   print(result)
#data = asarray([['This '], ['is '], ['a'],['document']])
sequence=input
size=4
index=0
ordinalen(sequence,size,index)

from sklearn.utils.sparsefuncs import inplace_column_scale
# one hot encoding function
def onehoten(sequence,size,index):
   doc=sentence_in_list(sequence,size,index)
   from numpy import asarray
   from sklearn.preprocessing import OneHotEncoder
   data = asarray(doc)
   print(data)
   encoder = OneHotEncoder(sparse=False)
   onehot = encoder.fit_transform(data)
   print(onehot)
#data = asarray([['This '], ['is '], ['a'],['document']])
sequence=input
size=2
index=0
onehoten(sequence,size,index)

# GloVe 2 function78uy78uy7u8y7u8y
import nltk
nltk.download('punkt')
def glovemod(sequences1,sequences2,size1,size2,index1,index2):
    lines=[spacings(sequences1,size1,index1),spacings(sequences2,size2,index2)]
    ! python -m pip install  glove-python-binary --verbose #if not available, it will install
    #pip install glove_python
    import nltk 
    # Tokenization
    from nltk.tokenize import sent_tokenize, word_tokenize
    word_tokens=[]
    i=0
    for line in lines:
       words = word_tokenize(line)
       word_tokens.insert(i,words)
       i=i+1
    print (word_tokens)
    #word_tokens has the words
    from glove import Corpus, Glove
    # creating a corpus object
    corpus = Corpus() 
    #training the corpus to generate the co occurence matrix which is used in GloVe
    corpus.fit(word_tokens, window=10)
    #creating a Glove object which will use the matrix created in the above lines to create embeddings
    #We can set the learning rate as it uses Gradient Descent and number of components
    glove = Glove(no_components=5, learning_rate=0.05)
    glove.fit(corpus.matrix, no_threads=4, verbose=True)
    glove.add_dictionary(corpus.dictionary)
    glove.save('glove.model')
    print(glove.dictionary)
    print(glove.word_vectors)
sequences1=input
sequences2=input1
size1=3
size2=2
index1=0
index2=0
glovemod(sequences1,sequences2,size1,size2,index1,index2)
#This statement will fetch out the encoding for a particular word from your data
#print(glove.word_vectors[glove.dictionary['This']])

#ElMo function
def elmo_mod(sequences,size,index):
  data=[spacings(sequences,size,index)]
  import tensorflow_hub as hub
  import tensorflow.compat.v1 as tf
  tf.disable_eager_execution()
  # elmo model
  elmo = hub.Module("https://tfhub.dev/google/elmo/3", trainable=True)
  # data
  model = elmo(data, as_dict= True)["elmo"]
  #init = tf.initialize_all_variables()
  init=tf.global_variables_initializer()
  sess = tf.Session()
  sess.run(init)
  print(sess.run(model))
#data
sequences=input
size=4
index=0
elmo_mod(sequences,size,index)

# BERT model function 
def bert(sequences,size,index):
   data=(spacings(sequences,size,index))
   ! pip install transformers
   from transformers import BertTokenizer, BertModel
   tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
   model = BertModel.from_pretrained("bert-base-uncased")
   inputs = tokenizer(data, return_tensors="pt")
   #print(inputs)
   outputs = model(**inputs)
   #print(outputs)
   last_hidden_states = outputs.last_hidden_state
   print(last_hidden_states)
sequences=input
size=4
index=0
bert(sequences,size,index)

#fasttext function
def fasttext(sequences1,sequences2,size1,size2,index1,index2):
    data=[[spacings(sequences1,size1,index1)],[spacings(sequences2,size2,index2)]]
    from gensim.models import FastText
    # train model
    model = FastText(data, min_count=1)
    # summarize the loaded model
    print(model)
    # summarize vocabulary
    words = list(model.wv.vocab)
    print(words)
    # access vector for one word
    print(model['TCT'])
    # save model
    model.save('model.bin')
    # load model
    new_model = FastText.load('model.bin')
    print(new_model)
sequences1=input
sequence2=input1
size1=3
size2=4
index1=0
index2=0
fasttext(sequences1,sequences2,size1,size2,index1,index2)